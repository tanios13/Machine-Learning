{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Solutions: U-Net\n",
    "$\\renewcommand{\\real}{\\mathbb{R}}$\n",
    "$\\renewcommand{\\xb}{\\mathbf{x}}$\n",
    "$\\renewcommand{\\yb}{\\mathbf{y}}$\n",
    "$\\renewcommand{\\zb}{\\mathbf{z}}$\n",
    "$\\renewcommand{\\wb}{\\mathbf{w}}$\n",
    "$\\renewcommand{\\Xb}{\\mathbf{X}}$\n",
    "$\\renewcommand{\\Lb}{\\mathbf{L}}$\n",
    "$\\DeclareMathOperator*{\\argmin}{argmin}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net is a convolutional neural network that was developed initially for biomedical image segmentation. This week, we will be working on implementing the U-Net architecture using PyTorch and performing an image segmentation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import helper\n",
    "import simulation\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will be working with a small dataset of synthetic images. Let us generate 2 images with their segmentation masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input images have shape (2, 1, 80, 80). Min value in image: 0. Max value in image: 255.\n",
      "Image masks have shape (2, 5, 80, 80). Min value in mask: 0.0. Max value in mask: 1.0.\n"
     ]
    }
   ],
   "source": [
    "# Generate some random images\n",
    "input_images, target_masks = simulation.generate_random_data(80, 80, count=2)\n",
    "\n",
    "print(\"Input images have shape {}. Min value in image: {}. Max value in image: {}.\".format(input_images.shape,input_images.min(),input_images.max()))\n",
    "print(\"Image masks have shape {}. Min value in mask: {}. Max value in mask: {}.\".format(target_masks.shape,target_masks.min(),target_masks.max()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input images have shape (2, 1, 80, 80):  2 for the number of images, 1 for the color channels (they are black and white in this case), and (80,80) for the image height and width.\n",
    "\n",
    "The images contain 6 different shapes. For each input image we have a segmentation mask, with which we encode what shape the pixels in the image segment.\n",
    "\n",
    "The corresponding image masks have shape (2, 6, 80, 80): 2 for the number of images, 6 for the one-hot encoding of what shape the pixel is a part of (see image below), and (80,80) for the image height and width.\n",
    "\n",
    "Let us plot the image and the corresponding segmentation mask to get a better idea of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Change channel-order and make 3 channels just for plotting\n",
    "input_images_rgb = [x.astype(np.uint8).repeat(3,axis=0).transpose(1,2,0) for x in input_images]\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [helper.masks_to_colorimg(x) for x in target_masks]\n",
    "\n",
    "# Left: Input image (black and white), Right: Target mask (6ch)\n",
    "helper.plot_side_by_side([input_images_rgb, target_masks_rgb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will be using PyTorch's Dataset class for preparing the data. Our SimDataset class will inherit from PyTorch's dataset class. We need to overwrite the functions `__len__(self)`, and `__getitem__(self, idx)`.\n",
    "\n",
    "*  `__len__(self)`: returns the size of the dataset.\n",
    "\n",
    "* `__getitem__(self, idx)`: returns the data sample at index `idx`.\n",
    "\n",
    "\n",
    "(Hint: The tutorial at https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class is very helpful and we encourage you to have a look at it. You can also add transformations for data augmentation etc. very easily!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimDataset(Dataset):\n",
    "    def __init__(self, count):\n",
    "        # We generate our data \n",
    "        self.input_images, self.target_masks = simulation.generate_random_data(80, 80, count=count)\n",
    "\n",
    "    def __len__(self):\n",
    "        #YOUR CODE HERE. Return the size of the dataset\n",
    "        return len(self.input_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #YOUR CODE HERE. Select data and mask at index idx.\n",
    "        image = self.input_images[idx]\n",
    "        mask = self.target_masks[idx]\n",
    "        \n",
    "        return [image, mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use PyTorch's DataLoader class. This is an iterator which allows us to batch the data, shuffle it and load it with multiprocessing workers in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the datasets and use PyTorch's DataLoader class.\n",
    "train_set = SimDataset(300)\n",
    "val_set = SimDataset(20)\n",
    "test_set = SimDataset(3)\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=20, shuffle=True)\n",
    "val_dataloader= DataLoader(val_set, batch_size=20, shuffle=True)\n",
    "test_dataloader= DataLoader(test_set, batch_size=3, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our Dataset and Dataloader, it is time to design our deep network! Since we are doing image segmentation, we choose to use a U-Net architecture here, which was first introduced in the paper \"U-Net: Convolutional Networks for Biomedical Image Segmentation\" ([link to paper](https://arxiv.org/abs/1505.04597) for the interested). The original architecture looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/unet.png\" width=800></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will quote the architecture description from the paper directly: \n",
    "\n",
    "> It consists of a contracting path (left side) and an expansive path (right side). \n",
    "\n",
    "> The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of **two 3x3 convolutions** (unpadded convolutions), each followed by a **rectified linear unit (ReLU)** and a **2x2 max pooling operation with stride 2** for downsampling. At each downsampling step we double the number of feature channels. \n",
    "\n",
    "\n",
    "> Every step in the expansive path consists of an **upsampling** of the feature map followed by a **2x2 convolution (“up-convolution”)** that halves the number of feature channels, a **concatenation with the correspondingly cropped feature map** from the contracting path, and **two 3x3 convolutions**, each followed by a **ReLU**. The cropping is necessary due to the loss of border pixels in every convolution. At the final layer a 1x1 convolution is used to map each 64- component feature vector to the desired number of classes. In total the network has 23 convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, we make matters slightly simpler by using padded convolutions with padding of size 1, so that the image doesn't become smaller after the convolution operations. This way, we do not have to crop the feature map from the contracting path for the concatenation. We will also build a slightly smaller network, since our images are not so large in size to begin with. The network we want to implement is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/unet-2.png\" width=800></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now lets code this! We would like to first create a module class for the double convolution operation (convolution -> batch norm -> ReLU -> convolution -> batch norm -> ReLU), since it is repeated several times in the architecture. This class inherits from PyTorch's module class. We use the nn.Sequential to define the layers in this module. \n",
    "\n",
    "Just to show you how this works, we wrote the class for a single convolution followed by a Sigmoid operation called `DummyConv`. We then use this `DummyConv` module three times in the `DummyNetwork`. Study how this works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyConv(nn.Module):\n",
    "    \"\"\"(convolution => sigmoid)\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):            \n",
    "        super().__init__()\n",
    "            \n",
    "        self.single_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.single_conv(x)\n",
    "\n",
    "    \n",
    "\n",
    "class DummyNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, n_channels):\n",
    "        super(DummyNetwork, self).__init__()\n",
    "        self.initial_layer = DummyConv(n_channels, 64)\n",
    "        self.second_layer = DummyConv(64, 128)\n",
    "        self.third_layer = DummyConv(128, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.initial_layer(x)\n",
    "        x2 = self.second_layer(x1)\n",
    "        x3 = self.third_layer(x2)\n",
    "        return x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to implement the `DoubleConv` module. Use the template above as help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => batch normalization => ReLU => convolution => batch normalization => ReLU)\n",
    "    Batch normalization is a technique for normalization over mini-batches. \n",
    "    You can use PyTorch's nn.BatchNorm2d layer \n",
    "    https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html.\n",
    "        \"\"\"\n",
    "    ## YOUR CODE HERE\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us implement the UNet class, as we did in previous week's exercise. We've started the implementation as a hint and left the rest for you to fill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.dc1 = DoubleConv(n_channels, 64)\n",
    "        self.mp1 = nn.MaxPool2d(2)\n",
    "        self.dc2 = DoubleConv(64, 128)\n",
    "        \n",
    "        ##YOUR CODE HERE\n",
    "        self.mp2 = nn.MaxPool2d(2)\n",
    "        self.dc3 = DoubleConv(128, 256)\n",
    "        self.mp3 = nn.MaxPool2d(2)\n",
    "        self.dc4 = DoubleConv(256, 512)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dc5 = DoubleConv(512, 256)\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dc6 = DoubleConv(256, 128)\n",
    "        ###\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dc7 = DoubleConv(128, 64)\n",
    "        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1 = self.dc1(x)\n",
    "        x2 = self.mp1(x1)\n",
    "        x2 = self.dc2(x2)\n",
    "        \n",
    "        ## YOUR CODE HERE\n",
    "        x3 = self.mp2(x2)\n",
    "        x3 = self.dc3(x3)\n",
    "        x4 = self.mp3(x3)\n",
    "        x4 = self.dc4(x4)\n",
    "\n",
    "        x = self.up1(x4)\n",
    "        x = torch.cat([x3, x], dim=1)\n",
    "        x = self.dc5(x)\n",
    "        x = self.up2(x)\n",
    "        x = torch.cat([x2, x], dim=1)\n",
    "        x = self.dc6(x)\n",
    "        ### \n",
    "#         \n",
    "        x = self.up3(x)\n",
    "        x = torch.cat([x1, x], dim=1)\n",
    "        x = self.dc7(x)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create this architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = UNet(n_channels=1, n_classes=5)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a sum of binary cross entropy loss and dice loss for our loss, since image segmentation is actually a classification problem (you can think of it as classifying what object the pixels belong to, if they belong to any.) You're already familiar with cross entropy loss from the lectures. Dice loss is a measure of overlap between the prediction and the ground truth labels and is also used for image segmentation tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    \n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    \n",
    "    return loss.mean()\n",
    "\n",
    "def calc_loss(pred, target):\n",
    "    bce_weight = 0.5\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
    "    pred = torch.sigmoid(pred)\n",
    "    dice = dice_loss(pred, target)\n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training code is given below. Fill in the missing part of the training loop! Call the `calc_loss` function above to calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, scheduler, num_epochs=10):\n",
    "    best_loss = 1e10\n",
    "    best_model_wts = None\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        since = time.time()\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(\"LR\", param_group['lr'])\n",
    "\n",
    "        ##### TRAINING:\n",
    "        model.train()  # Set model to training mode\n",
    "        epoch_loss = 0\n",
    "        epoch_samples = 0\n",
    "        #load the images and masks\n",
    "        for bi, (inputs, labels) in enumerate(train_dataloader):\n",
    "            print(f\"\\rProcessing batch {bi}/\"\n",
    "                  f\"{len(train_dataloader) - 1}\", end='')\n",
    "            inputs = inputs.to(device).float()\n",
    "            labels = labels.to(device).float()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                ## YOUR CODE HERE\n",
    "                outputs = model(inputs)\n",
    "                loss = calc_loss(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # statistics\n",
    "                epoch_samples +=  inputs.size(0)\n",
    "                epoch_loss += loss.data.cpu().numpy() * inputs.size(0)\n",
    "        print(\"Training epoch loss: {}\".format(epoch_loss/epoch_samples))\n",
    "        scheduler.step()\n",
    "\n",
    "        ##### VALIDATION:\n",
    "        model.eval()   # Set model to evaluate mode\n",
    "        epoch_loss = 0\n",
    "        epoch_samples = 0\n",
    "        for inputs, labels in val_dataloader:\n",
    "            inputs = inputs.to(device).float()\n",
    "            labels = labels.to(device).float()\n",
    "\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = model(inputs)\n",
    "                loss = calc_loss(outputs, labels)\n",
    "\n",
    "                # statistics\n",
    "                epoch_loss += loss.data.cpu().numpy() * inputs.size(0)\n",
    "                epoch_samples +=  inputs.size(0)\n",
    "        print(\"Val epoch loss: {}\".format(epoch_loss/epoch_samples))\n",
    "\n",
    "        # save the model if the loss is the best\n",
    "        if epoch_loss/epoch_samples < best_loss:\n",
    "            print(\"saving best model\")\n",
    "            best_loss = epoch_loss/epoch_samples\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, it is time to train! \n",
    "\n",
    "First we will create the optimizer. This time we will use the Adam optimizer ([optim.Adam](https://pytorch.org/docs/stable/optim.html)). Set the learning rate to 1e-2.\n",
    "\n",
    "We've also created a learning rate scheduler for you, which gradually reduces the learning rate during training.\n",
    "\n",
    "If you have a GPU with CUDA support, this should be pretty fast! However, it might take a while if you only train on the CPU. If this is the case, try it with a reduced number of epochs first. (You should be seeing some segmented objects after 3 epochs (but keep in mind that at this point the classification will be likely wrong, i.e. visually the colors will not match)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "LR 0.01\n",
      "Processing batch 14/14Training epoch loss: 0.5928093075752259\n",
      "Val epoch loss: 0.7918888330459595\n",
      "saving best model\n",
      "0m 42s\n",
      "Epoch 1/9\n",
      "----------\n",
      "LR 0.01\n",
      "Processing batch 14/14Training epoch loss: 0.3868025759855906\n",
      "Val epoch loss: 0.46131983399391174\n",
      "saving best model\n",
      "0m 36s\n",
      "Epoch 2/9\n",
      "----------\n",
      "LR 0.01\n",
      "Processing batch 14/14Training epoch loss: 0.30996406078338623\n",
      "Val epoch loss: 0.28438031673431396\n",
      "saving best model\n",
      "0m 35s\n",
      "Epoch 3/9\n",
      "----------\n",
      "LR 0.009000000000000001\n",
      "Processing batch 10/14"
     ]
    }
   ],
   "source": [
    "model = UNet(n_channels=1, n_classes=5)\n",
    "model = model.to(device)\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=1e-2)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=3, gamma=0.9)\n",
    "train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how well we do on the test data! We plot the ground truth masks and the predicted masks side by side below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first batch\n",
    "inputs, labels = next(iter(test_dataloader))\n",
    "inputs = inputs.to(device).float()\n",
    "labels = labels.to(device).float()\n",
    "\n",
    "# Predict\n",
    "pred = model(inputs)\n",
    "pred = torch.sigmoid(pred)\n",
    "pred = pred.data.cpu().numpy()\n",
    "\n",
    "# Change channel-order and make 3 channels for matplot\n",
    "input_images_rgb = [x.repeat(3,axis=0).transpose(1,2,0).astype(np.uint8) for x in inputs.cpu().numpy()]\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [helper.masks_to_colorimg(x) for x in labels.cpu().numpy()]\n",
    "pred_rgb = [helper.masks_to_colorimg(x) for x in pred]\n",
    "helper.plot_side_by_side([target_masks_rgb, pred_rgb])\n",
    "helper.plot_side_by_side([input_images_rgb, pred_rgb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acknowledgements:**\n",
    "\n",
    "This notebook contains a mix of usuyama's https://github.com/usuyama/pytorch-unet and milesial's https://github.com/milesial/Pytorch-UNet code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
